{
 "metadata": {
  "name": "",
  "signature": "sha256:9baa34067c4343bb7b9a79c4201824c29b6dfa0f1034d0b9c03fe16481f53a7f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "# set up and importing \n",
      "import sys\n",
      "import pickle\n",
      "sys.path.append(\"../tools/\")\n",
      "from feature_format import featureFormat, targetFeatureSplit\n",
      "from tester import dump_classifier_and_data\n",
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Task 1: Select what features you'll use.\n",
      "features_list is a list of strings, each of which is a feature name.   \n",
      "The first feature must be \"poi\".   \n",
      "You will need to use more features    \n",
      "How do I Complete this Project?\n",
      "\n",
      "financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
      "\n",
      "email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is \u2018email_address\u2019, which is a text string)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_list = ['poi','salary' , 'deferral_payments', 'total_payments', 'loan_advances',\n",
      "                 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', \n",
      "                 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
      "                 'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person', \n",
      "                 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
      "print len(features_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load the dictionary containing the dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
      "    data_dict = pickle.load(data_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Exploration\n",
      "Student response addresses the most important characteristics of the dataset and uses these characteristics to inform their analysis. Important characteristics include:\n",
      "total number of data points   \n",
      "allocation across classes (POI/non-POI)   \n",
      "number of features   \n",
      "are there features with many missing values? etc.    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## total number of data points"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "146\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The person names in the data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for person in data_dict.keys():\n",
      "    print person"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "METTS MARK\n",
        "BAXTER JOHN C\n",
        "ELLIOTT STEVEN\n",
        "CORDES WILLIAM R\n",
        "HANNON KEVIN P\n",
        "MORDAUNT KRISTINA M\n",
        "MEYER ROCKFORD G\n",
        "MCMAHON JEFFREY\n",
        "HORTON STANLEY C\n",
        "PIPER GREGORY F\n",
        "HUMPHREY GENE E\n",
        "UMANOFF ADAM S\n",
        "BLACHMAN JEREMY M\n",
        "SUNDE MARTIN\n",
        "GIBBS DANA R\n",
        "LOWRY CHARLES P\n",
        "COLWELL WESLEY\n",
        "MULLER MARK S\n",
        "JACKSON CHARLENE R\n",
        "WESTFAHL RICHARD K\n",
        "WALTERS GARETH W\n",
        "WALLS JR ROBERT H\n",
        "KITCHEN LOUISE\n",
        "CHAN RONNIE\n",
        "BELFER ROBERT\n",
        "SHANKMAN JEFFREY A\n",
        "WODRASKA JOHN\n",
        "BERGSIEKER RICHARD P\n",
        "URQUHART JOHN A\n",
        "BIBI PHILIPPE A\n",
        "RIEKER PAULA H\n",
        "WHALEY DAVID A\n",
        "BECK SALLY W\n",
        "HAUG DAVID L\n",
        "ECHOLS JOHN B\n",
        "MENDELSOHN JOHN\n",
        "HICKERSON GARY J\n",
        "CLINE KENNETH W\n",
        "LEWIS RICHARD\n",
        "HAYES ROBERT E\n",
        "MCCARTY DANNY J\n",
        "KOPPER MICHAEL J\n",
        "LEFF DANIEL P\n",
        "LAVORATO JOHN J\n",
        "BERBERIAN DAVID\n",
        "DETMERING TIMOTHY J\n",
        "WAKEHAM JOHN\n",
        "POWERS WILLIAM\n",
        "GOLD JOSEPH\n",
        "BANNANTINE JAMES M\n",
        "DUNCAN JOHN H\n",
        "SHAPIRO RICHARD S\n",
        "SHERRIFF JOHN R\n",
        "SHELBY REX\n",
        "LEMAISTRE CHARLES\n",
        "DEFFNER JOSEPH M\n",
        "KISHKILL JOSEPH G\n",
        "WHALLEY LAWRENCE G\n",
        "MCCONNELL MICHAEL S\n",
        "PIRO JIM\n",
        "DELAINEY DAVID W\n",
        "SULLIVAN-SHAKLOVITZ COLLEEN\n",
        "WROBEL BRUCE\n",
        "LINDHOLM TOD A\n",
        "MEYER JEROME J\n",
        "LAY KENNETH L\n",
        "BUTTS ROBERT H\n",
        "OLSON CINDY K\n",
        "MCDONALD REBECCA\n",
        "CUMBERLAND MICHAEL S\n",
        "GAHN ROBERT S\n",
        "MCCLELLAN GEORGE\n",
        "HERMANN ROBERT J\n",
        "SCRIMSHAW MATTHEW\n",
        "GATHMANN WILLIAM D\n",
        "HAEDICKE MARK E\n",
        "BOWEN JR RAYMOND M\n",
        "GILLIS JOHN\n",
        "FITZGERALD JAY L\n",
        "MORAN MICHAEL P\n",
        "REDMOND BRIAN L\n",
        "BAZELIDES PHILIP J\n",
        "BELDEN TIMOTHY N\n",
        "DURAN WILLIAM D\n",
        "THORN TERENCE H\n",
        "FASTOW ANDREW S\n",
        "FOY JOE\n",
        "CALGER CHRISTOPHER F\n",
        "RICE KENNETH D\n",
        "KAMINSKI WINCENTY J\n",
        "LOCKHART EUGENE E\n",
        "COX DAVID\n",
        "OVERDYKE JR JERE C\n",
        "PEREIRA PAULO V. FERRAZ\n",
        "STABLER FRANK\n",
        "SKILLING JEFFREY K\n",
        "BLAKE JR. NORMAN P\n",
        "SHERRICK JEFFREY B\n",
        "PRENTICE JAMES\n",
        "GRAY RODNEY\n",
        "PICKERING MARK R\n",
        "THE TRAVEL AGENCY IN THE PARK\n",
        "NOLES JAMES L\n",
        "KEAN STEVEN J\n",
        "TOTAL\n",
        "FOWLER PEGGY\n",
        "WASAFF GEORGE\n",
        "WHITE JR THOMAS E\n",
        "CHRISTODOULOU DIOMEDES\n",
        "ALLEN PHILLIP K\n",
        "SHARP VICTORIA T\n",
        "JAEDICKE ROBERT\n",
        "WINOKUR JR. HERBERT S\n",
        "BROWN MICHAEL\n",
        "BADUM JAMES P\n",
        "HUGHES JAMES A\n",
        "REYNOLDS LAWRENCE\n",
        "DIMICHELE RICHARD G\n",
        "BHATNAGAR SANJAY\n",
        "CARTER REBECCA C\n",
        "BUCHANAN HAROLD G\n",
        "YEAP SOON\n",
        "MURRAY JULIA H\n",
        "GARLAND C KEVIN\n",
        "DODSON KEITH\n",
        "YEAGER F SCOTT\n",
        "HIRKO JOSEPH\n",
        "DIETRICH JANET R\n",
        "DERRICK JR. JAMES V\n",
        "FREVERT MARK A\n",
        "PAI LOU L\n",
        "BAY FRANKLIN R\n",
        "HAYSLETT RODERICK J\n",
        "FUGH JOHN L\n",
        "FALLON JAMES B\n",
        "KOENIG MARK E\n",
        "SAVAGE FRANK\n",
        "IZZO LAWRENCE L\n",
        "TILNEY ELIZABETH A\n",
        "MARTIN AMANDA K\n",
        "BUY RICHARD B\n",
        "GRAMM WENDY L\n",
        "CAUSEY RICHARD A\n",
        "TAYLOR MITCHELL S\n",
        "DONAHUE JR JEFFREY M\n",
        "GLISAN JR BEN F\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npoi = 0\n",
      "for p in data_dict.values():\n",
      "    if p['poi']:\n",
      "        npoi += 1\n",
      "print npoi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are 18 person of interest out of the 145 total persons"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "round(18.0/146.0*100,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "12.33"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NaNInFeatures = [0 for i in range(len(features_list))]\n",
      "for i, person in enumerate(data_dict.values()):\n",
      "    for j, feature in enumerate(features_list):\n",
      "        if person[feature] == 'NaN':\n",
      "            NaNInFeatures[j] += 1\n",
      "\n",
      "for i, feature in enumerate(features_list):\n",
      "    print feature, NaNInFeatures[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "poi 0\n",
        "salary 51\n",
        "deferral_payments 107\n",
        "total_payments 21\n",
        "loan_advances 142\n",
        "bonus 64\n",
        "restricted_stock_deferred 128\n",
        "deferred_income 97\n",
        "total_stock_value 20\n",
        "expenses 51\n",
        "exercised_stock_options 44\n",
        "other 53\n",
        "long_term_incentive 80\n",
        "restricted_stock 36\n",
        "director_fees 129\n",
        "to_messages 60\n",
        "from_poi_to_this_person 60\n",
        "from_messages 60\n",
        "from_this_person_to_poi 60\n",
        "shared_receipt_with_poi 60\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Task 2: Remove outliers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It appears that there are a mistake entry that is 'TOTAL'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dict.pop('TOTAL')\n",
      "len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "145"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Task 3: Create new features   \n",
      "### Store to my_dataset for easy export below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_dataset = data_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At least one new feature is implemented.    \n",
      "Justification for that feature is provided in the written response, and the effect of that feature on the final algorithm performance is tested.    \n",
      "\n",
      "Create two new features, one is `to_poi_message_ratio` and the other is `from_poi_to_this_person`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for person in my_dataset.values():\n",
      "    person['to_poi_message_ratio'] = 'NaN'\n",
      "    person['from_poi_message_ratio'] = 'NaN'\n",
      "    person['deferral_ratio'] = 'NaN'\n",
      "    person['deferred_stock_ratio'] = 'NaN'\n",
      "    person['emails_with_poi_ratio'] = 'NaN'\n",
      "    if person['to_messages'] != 'NaN' and person['from_this_person_to_poi'] != 'NaN':\n",
      "        person['to_poi_message_ratio'] = 1.0*person['from_this_person_to_poi']/person['to_messages']  \n",
      "    if person['from_poi_to_this_person'] != 'NaN' and person['from_messages'] != 'NaN':\n",
      "        person['from_poi_message_ratio'] = 1.0*person['from_poi_to_this_person']/person['from_messages']\n",
      "    if person['restricted_stock_deferred'] != 'NaN' and person['total_stock_value'] != 'NaN':\n",
      "        person['deferred_stock_ratio'] = 1.0*person['restricted_stock_deferred']/person['total_stock_value']\n",
      "    if person['deferral_payments'] != 'NaN' and person['total_payments'] != 'NaN':\n",
      "        person['deferral_ratio'] = person['deferral_payments']/person['total_payments']\n",
      "    if person['to_poi_message_ratio'] != 'NaN' and person['from_poi_message_ratio'] != 'NaN':\n",
      "        person['emails_with_poi_ratio'] = 1.0*(person['from_this_person_to_poi'] + person['from_poi_to_this_person'])/(person['to_messages'] + person['from_messages'])\n",
      "    \n",
      "features_list.extend(['to_poi_message_ratio', 'from_poi_message_ratio', 'deferral_ratio', 'deferred_stock_ratio', 'emails_with_poi_ratio'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Extract features and labels from dataset for local testing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = featureFormat(my_dataset, features_list)\n",
      "labels, features = targetFeatureSplit(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(features_list)\n",
      "for f in features_list:\n",
      "    print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25\n",
        "poi\n",
        "salary\n",
        "deferral_payments\n",
        "total_payments\n",
        "loan_advances\n",
        "bonus\n",
        "restricted_stock_deferred\n",
        "deferred_income\n",
        "total_stock_value\n",
        "expenses\n",
        "exercised_stock_options\n",
        "other\n",
        "long_term_incentive\n",
        "restricted_stock\n",
        "director_fees\n",
        "to_messages\n",
        "from_poi_to_this_person\n",
        "from_messages\n",
        "from_this_person_to_poi\n",
        "shared_receipt_with_poi\n",
        "to_poi_message_ratio\n",
        "from_poi_message_ratio\n",
        "deferral_ratio\n",
        "deferred_stock_ratio\n",
        "emails_with_poi_ratio\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intelligently select features (related mini-project: Lesson 11)   \n",
      "Univariate or recursive feature selection is deployed,    \n",
      "or features are selected by hand      \n",
      "(different combinations of features are attempted, and the performance is documented for each one).     \n",
      "Features that are selected are reported and the number of features selected is justified.      \n",
      "For an algorithm that supports getting the feature importances      \n",
      "(e.g. decision tree) or feature scores (e.g. SelectKBest), those are documented as well.     "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (i in range(len()))\n",
      "selector = SelectKBest(f_classif, k=2)\n",
      "featuresselector.fit(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "SelectPercentile(percentile=10,\n",
        "         score_func=<function f_classif at 0x0000000008D456D8>)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(features[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selected_features = [features_list[1:][i] for i, s in enumerate(scores) if s > 0.05]\n",
      "selected_features = ['poi'] + selected_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(selected_features)\n",
      "for f in selected_features:\n",
      "    print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9\n",
        "poi\n",
        "deferral_payments\n",
        "restricted_stock_deferred\n",
        "director_fees\n",
        "to_messages\n",
        "from_messages\n",
        "from_this_person_to_poi\n",
        "deferral_ratio\n",
        "deferred_stock_ratio\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = featureFormat(my_dataset, selected_features)\n",
      "labels, features = targetFeatureSplit(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 4: Try a varity of classifiers\n",
      "### Please name your classifier clf for easy export below.\n",
      "### Note that if you want to do PCA or other multi-stage operations,\n",
      "### you'll need to use Pipelines. For more info:\n",
      "### http://scikit-learn.org/stable/modules/pipeline.html\n",
      "\n",
      "# Provided to give you a starting point. Try a variety of classifiers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_retain_ratio(labels, features, test_size = 0.333, random_state=1126):\n",
      "    features_0 = []\n",
      "    features_1 = []\n",
      "    labels_0 = []\n",
      "    labels_1 = []\n",
      "    for i, f in enumerate(features):\n",
      "        if labels[i] == 0:\n",
      "            features_0.append(f)\n",
      "            labels_0.append(0.0)\n",
      "        else:\n",
      "            features_1.append(f)\n",
      "            labels_1.append(1.0)\n",
      "    features0_train, features0_test,labels0_train, labels0_test = train_test_split(features_0, labels_0, test_size=test_size, \n",
      "                                                                                   random_state=random_state)\n",
      "    features1_train, features1_test,labels1_train, labels1_test = train_test_split(features_1, labels_1, test_size=test_size, \n",
      "                                                                                   random_state=random_state)\n",
      "    \n",
      "    return features0_train + features1_train, features0_test + features1_test, labels0_train + labels1_train, labels0_test + labels1_test\n",
      "\n",
      "features_train, features_test,labels_train, labels_test = train_test_split(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adaBoostClf = AdaBoostClassifier()\n",
      "adaBoostClf.fit(features_train, labels_train)\n",
      "adaBoostPred = adaBoostClf.predict(features_test)\n",
      "print \"adaBoost precision: \", precision_score(labels_test, adaBoostPred)\n",
      "print \"adaBoost Recall: \", recall_score(labels_test, adaBoostPred)\n",
      "rfClf = RandomForestClassifier()\n",
      "rfClf.fit(features_train, labels_train)\n",
      "rfPred = rfClf.predict(features_test)\n",
      "print \"randomForest precision: \", precision_score(labels_test, rfPred)\n",
      "print \"randomForest Recall: \", recall_score(labels_test, rfPred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "adaBoost precision:  0.666666666667\n",
        "adaBoost Recall:  0.5\n",
        "randomForest precision:  0.0\n",
        "randomForest Recall:  0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\Ryan\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Parameter tuning for random forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(sum(labels_train))\n",
      "print(sum(labels_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11.0\n",
        "4.0\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SCV = StratifiedShuffleSplit(labels, 100, test_size=0.3, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tuned_parameters = {'n_estimators': [10,100,200], 'min_samples_split': [1,2,4], 'max_features': [2,4,6]}\n",
      "print(\"# Tuning hyper-parameters for recall\")\n",
      "RF = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=SCV, scoring = 'recall')\n",
      "RF.fit(features, labels)\n",
      "print(\"Best parameters are:\")\n",
      "print(RF.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# Tuning hyper-parameters for recall\n",
        "Best parameters are:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'max_features': 4, 'min_samples_split': 4, 'n_estimators': 10}\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Parameter tuning for AdaBoost"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tuned_parameters = {'n_estimators': [10,25,50,100,200], 'learning_rate': [0.2,0.4,0.6,0.8,1]}\n",
      "print(\"# Tuning hyper-parameters for recall\")\n",
      "Adaboost = GridSearchCV(AdaBoostClassifier(), tuned_parameters, cv=5, scoring = 'recall')\n",
      "Adaboost.fit(features_train, labels_train)\n",
      "print(\"Best parameters are:\")\n",
      "print(Adaboost.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# Tuning hyper-parameters for recall\n",
        "Best parameters are:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'n_estimators': 200, 'learning_rate': 0.8}\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
      "### using our testing script. Check the tester.py script in the final project\n",
      "### folder for details on the evaluation method, especially the test_classifier\n",
      "### function. Because of the small size of the dataset, the script uses\n",
      "### stratified shuffle split cross validation. For more info: \n",
      "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Clf = AdaBoostClassifier(n_estimators = 25, learning_rate = 0.6)\n",
      "Clf.fit(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
        "          learning_rate=0.6, n_estimators=25, random_state=None)"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Clf =  RandomForestClassifier(max_features = 6, min_samples_split = 4, n_estimators = 10)\n",
      "Clf.fit(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "            max_depth=None, max_features=6, max_leaf_nodes=None,\n",
        "            min_samples_leaf=1, min_samples_split=4,\n",
        "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0,\n",
        "            warm_start=False)"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
      "### check your results. You do not need to change anything below, but make sure\n",
      "### that the version of poi_id.py that you submit can be run on its own and\n",
      "### generates the necessary .pkl files for validating your results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dump_classifier_and_data(Clf, my_dataset, features_list)\n",
      "dump_classifier_and_data(Clf, my_dataset, selected_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Clf = Adaboost.best_estimator_\n",
      "#Clf = RF.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}